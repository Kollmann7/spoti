{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db335711",
   "metadata": {},
   "source": [
    "# NER Inference on Job Descriptions\n",
    "\n",
    "Use the trained NER model to extract skills from job descriptions.\n",
    "- Load the custom NER model from `artifacts/ner_custom`\n",
    "- Process job descriptions from the extraction CSV\n",
    "- Extract and categorize skills using NER\n",
    "- Save results with entity annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25daf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed successfully\n",
      "SpaCy version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import datetime as dt\n",
    "\n",
    "# SpaCy imports\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "print(\"Imports completed successfully\")\n",
    "print(f\"SpaCy version: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0c504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: artifacts\\ner_custom\n",
      "Output directory: output\n",
      "Results directory: results\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "OUTPUT_DIR = Path('output')\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_PATH = ARTIFACTS_DIR / 'ner_custom'\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e79d4",
   "metadata": {},
   "source": [
    "## 1. Load the Trained NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b40db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model not found at artifacts\\ner_custom\n",
      "Falling back to standard French model...\n",
      "No French model available. Installing fr_core_news_sm...\n",
      "\n",
      "Model pipeline: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "\n",
      "Model pipeline: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Try to load the custom NER model\n",
    "try:\n",
    "    if MODEL_PATH.exists():\n",
    "        nlp = spacy.load(MODEL_PATH)\n",
    "        print(f\"Loaded custom NER model from {MODEL_PATH}\")\n",
    "        \n",
    "        # Check if model has NER component\n",
    "        if 'ner' in nlp.pipe_names:\n",
    "            ner = nlp.get_pipe('ner')\n",
    "            labels = list(ner.labels)\n",
    "            print(f\"Available entity labels: {labels}\")\n",
    "        else:\n",
    "            print(\"WARNING: No NER component found in the model\")\n",
    "    else:\n",
    "        print(f\"Custom model not found at {MODEL_PATH}\")\n",
    "        print(\"Falling back to standard French model...\")\n",
    "        \n",
    "        try:\n",
    "            nlp = spacy.load('fr_core_news_sm')\n",
    "            print(\"Loaded standard French model\")\n",
    "        except OSError:\n",
    "            print(\"No French model available. Installing fr_core_news_sm...\")\n",
    "            os.system('python -m spacy download fr_core_news_sm')\n",
    "            nlp = spacy.load('fr_core_news_sm')\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Using blank French model...\")\n",
    "    nlp = spacy.blank('fr')\n",
    "\n",
    "print(f\"\\nModel pipeline: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aa319",
   "metadata": {},
   "source": [
    "## 2. Load Job Descriptions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f06bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: output\\pdf_extraction_results_20250830_131547_with_content.csv\n",
      "Loaded 998 documents\n",
      "Columns: ['file_name', 'file_path', 'file_size_kb', 'extraction_timestamp', 'content_type', 'page_count', 'extracted_text', 'has_text', 'has_images', 'image_count', 'word_count', 'char_count', 'paragraph_count', 'line_count', 'error']\n",
      "Using text column: extracted_text\n",
      "Documents with text: 997\n"
     ]
    }
   ],
   "source": [
    "# Find the latest extraction CSV\n",
    "csv_files = list(OUTPUT_DIR.glob('*with_content*.csv'))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No extraction CSV found in {OUTPUT_DIR}\")\n",
    "\n",
    "latest_csv = sorted(csv_files)[-1]\n",
    "print(f\"Loading data from: {latest_csv}\")\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(latest_csv)\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Identify text column\n",
    "if 'extracted_text' in df.columns:\n",
    "    text_column = 'extracted_text'\n",
    "elif 'content' in df.columns:\n",
    "    text_column = 'content'\n",
    "else:\n",
    "    text_column = df.columns[-1]\n",
    "\n",
    "print(f\"Using text column: {text_column}\")\n",
    "print(f\"Documents with text: {df[text_column].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff4dbd",
   "metadata": {},
   "source": [
    "## 3. Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8c54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_ner(text):\n",
    "    \"\"\"Clean text for better NER performance.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters that might confuse NER\n",
    "    text = re.sub(r'[^\\\\w\\\\s.,!?;:()/\\\\-]', ' ', text)\n",
    "    \n",
    "    # Normalize multiple punctuation\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_entities_from_text(text, nlp_model):\n",
    "    \"\"\"Extract entities from text using the NER model.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char,\n",
    "            'confidence': getattr(ent, 'confidence', None)\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def chunk_long_text(text, max_length=1000000):\n",
    "    \"\"\"Split very long texts into chunks for processing.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    sentences = text.split('. ')\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_length:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f5a1a",
   "metadata": {},
   "source": [
    "## 4. Process Documents with NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a14fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 997 documents with text\n",
      "Cleaning text...\n",
      "\n",
      "Cleaning text...\n",
      "After filtering short texts: 995 documents\n",
      "Extracting entities with NER...\n",
      "After filtering short texts: 995 documents\n",
      "Extracting entities with NER...\n",
      "  Processed 100/995 documents\n",
      "  Processed 100/995 documents\n",
      "  Processed 200/995 documents\n",
      "  Processed 200/995 documents\n",
      "  Processed 300/995 documents\n",
      "  Processed 300/995 documents\n",
      "  Processed 400/995 documents\n",
      "  Processed 400/995 documents\n",
      "  Processed 500/995 documents\n",
      "  Processed 500/995 documents\n",
      "  Processed 600/995 documents\n",
      "  Processed 600/995 documents\n",
      "  Processed 700/995 documents\n",
      "  Processed 700/995 documents\n",
      "  Processed 800/995 documents\n",
      "  Processed 800/995 documents\n",
      "  Processed 900/995 documents\n",
      "  Processed 900/995 documents\n",
      "\n",
      "Processing completed!\n",
      "Documents with entities: 704\n",
      "Average entities per document: 1.61\n",
      "Processing errors: 0\n",
      "\n",
      "Processing completed!\n",
      "Documents with entities: 704\n",
      "Average entities per document: 1.61\n",
      "Processing errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter documents with text\n",
    "df_with_text = df[df[text_column].notna() & (df[text_column].str.len() > 10)].copy()\n",
    "print(f\"Processing {len(df_with_text)} documents with text\")\n",
    "\n",
    "# Clean text\n",
    "print(\"Cleaning text...\")\n",
    "df_with_text['clean_text'] = df_with_text[text_column].apply(clean_text_for_ner)\n",
    "\n",
    "# Filter out very short texts\n",
    "df_with_text = df_with_text[df_with_text['clean_text'].str.len() >= 50].copy()\n",
    "print(f\"After filtering short texts: {len(df_with_text)} documents\")\n",
    "\n",
    "# Process with NER\n",
    "print(\"Extracting entities with NER...\")\n",
    "all_entities = []\n",
    "processing_errors = []\n",
    "\n",
    "for idx, row in df_with_text.iterrows():\n",
    "    try:\n",
    "        text = row['clean_text']\n",
    "        \n",
    "        # Handle very long texts by chunking\n",
    "        chunks = chunk_long_text(text)\n",
    "        doc_entities = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk_entities = extract_entities_from_text(chunk, nlp)\n",
    "            doc_entities.extend(chunk_entities)\n",
    "        \n",
    "        all_entities.append(doc_entities)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df_with_text)} documents\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {idx}: {e}\")\n",
    "        all_entities.append([])\n",
    "        processing_errors.append((idx, str(e)))\n",
    "\n",
    "df_with_text['entities'] = all_entities\n",
    "df_with_text['num_entities'] = df_with_text['entities'].apply(len)\n",
    "\n",
    "print(f\"\\nProcessing completed!\")\n",
    "print(f\"Documents with entities: {(df_with_text['num_entities'] > 0).sum()}\")\n",
    "print(f\"Average entities per document: {df_with_text['num_entities'].mean():.2f}\")\n",
    "print(f\"Processing errors: {len(processing_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a14dc",
   "metadata": {},
   "source": [
    "## 5. Analyze Extracted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff1b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Analysis:\n",
      "==================================================\n",
      "\n",
      "Total entities extracted: 1599\n",
      "Unique entity texts: 388\n",
      "\n",
      "Entity types distribution:\n",
      "  LOC            :  725 ( 45.3%)\n",
      "  PER            :  504 ( 31.5%)\n",
      "  MISC           :  262 ( 16.4%)\n",
      "  ORG            :  108 (  6.8%)\n",
      "\n",
      "Most frequent entities (top 20):\n",
      "  /                   :  81\n",
      "  s.                  :  59\n",
      "  www                 :  54\n",
      "  ---                 :  42\n",
      "  .                   :  40\n",
      "  s.                  :  37\n",
      "  s                   :  36\n",
      "  -                   :  34\n",
      "  s.                         :  32\n",
      "  ss                  :  28\n",
      "  !                   :  27\n",
      "  -                   :  26\n",
      "  -                   :  25\n",
      "  s.                  :  24\n",
      "  ./                  :  24\n",
      "  ---        ---      :  21\n",
      "  s                   :  20\n",
      "  ---        ---      :  18\n",
      "  s                   :  17\n",
      "  s                   :  16\n"
     ]
    }
   ],
   "source": [
    "# Collect all entities for analysis\n",
    "all_entity_texts = []\n",
    "entity_label_counts = Counter()\n",
    "entity_text_counts = Counter()\n",
    "\n",
    "for entities_list in all_entities:\n",
    "    for entity in entities_list:\n",
    "        entity_label_counts[entity['label']] += 1\n",
    "        entity_text_counts[entity['text'].lower()] += 1\n",
    "        all_entity_texts.append(entity['text'])\n",
    "\n",
    "print(\"Entity Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal entities extracted: {len(all_entity_texts)}\")\n",
    "print(f\"Unique entity texts: {len(entity_text_counts)}\")\n",
    "\n",
    "print(\"\\nEntity types distribution:\")\n",
    "for label, count in entity_label_counts.most_common():\n",
    "    percentage = (count / len(all_entity_texts)) * 100\n",
    "    print(f\"  {label:<15}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nMost frequent entities (top 20):\")\n",
    "for entity_text, count in entity_text_counts.most_common(20):\n",
    "    print(f\"  {entity_text:<20}: {count:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73340bf5",
   "metadata": {},
   "source": [
    "## 6. Prepare Results for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a35faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame created: 995 rows, 12 columns\n",
      "Entity-level DataFrame created: 1599 rows\n"
     ]
    }
   ],
   "source": [
    "# Create detailed results DataFrame\n",
    "results_detailed = []\n",
    "\n",
    "for idx, row in df_with_text.iterrows():\n",
    "    base_info = {\n",
    "        'filename': row.get('filename', f'doc_{idx}'),\n",
    "        'text_length': len(row['clean_text']),\n",
    "        'num_entities': row['num_entities']\n",
    "    }\n",
    "    \n",
    "    # Add metadata if available\n",
    "    for col in ['page_count', 'extraction_time']:\n",
    "        if col in row:\n",
    "            base_info[col] = row[col]\n",
    "    \n",
    "    # Group entities by label\n",
    "    entities_by_label = defaultdict(list)\n",
    "    for entity in row['entities']:\n",
    "        entities_by_label[entity['label']].append(entity['text'])\n",
    "    \n",
    "    # Add entity information\n",
    "    for label in entity_label_counts.keys():\n",
    "        base_info[f'num_{label.lower()}'] = len(entities_by_label[label])\n",
    "        base_info[f'{label.lower()}_entities'] = ', '.join(set(entities_by_label[label]))\n",
    "    \n",
    "    results_detailed.append(base_info)\n",
    "\n",
    "df_results = pd.DataFrame(results_detailed)\n",
    "print(f\"Results DataFrame created: {len(df_results)} rows, {len(df_results.columns)} columns\")\n",
    "\n",
    "# Create entity-level results\n",
    "entity_results = []\n",
    "\n",
    "for idx, row in df_with_text.iterrows():\n",
    "    filename = row.get('filename', f'doc_{idx}')\n",
    "    \n",
    "    for entity in row['entities']:\n",
    "        entity_results.append({\n",
    "            'filename': filename,\n",
    "            'entity_text': entity['text'],\n",
    "            'entity_label': entity['label'],\n",
    "            'start_char': entity['start'],\n",
    "            'end_char': entity['end'],\n",
    "            'confidence': entity.get('confidence')\n",
    "        })\n",
    "\n",
    "df_entities = pd.DataFrame(entity_results)\n",
    "print(f\"Entity-level DataFrame created: {len(df_entities)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb05b93",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add7c36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-level results saved: results\\ner_results_20250831_135758.csv\n",
      "Entity-level results saved: results\\ner_entities_20250831_135758.csv\n",
      "Analysis summary saved: results\\ner_summary_20250831_135758.json\n",
      "\n",
      "NER inference completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate timestamp\n",
    "timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save document-level results\n",
    "results_file = RESULTS_DIR / f'ner_results_{timestamp}.csv'\n",
    "df_results.to_csv(results_file, index=False, encoding='utf-8')\n",
    "print(f\"Document-level results saved: {results_file}\")\n",
    "\n",
    "# Save entity-level results\n",
    "entities_file = RESULTS_DIR / f'ner_entities_{timestamp}.csv'\n",
    "df_entities.to_csv(entities_file, index=False, encoding='utf-8')\n",
    "print(f\"Entity-level results saved: {entities_file}\")\n",
    "\n",
    "# Save analysis summary\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'source_file': str(latest_csv),\n",
    "    'model_used': str(MODEL_PATH) if MODEL_PATH.exists() else 'fr_core_news_sm',\n",
    "    'total_documents': int(len(df_with_text)),\n",
    "    'documents_with_entities': int((df_with_text['num_entities'] > 0).sum()),\n",
    "    'total_entities': int(len(all_entity_texts)),\n",
    "    'unique_entities': int(len(entity_text_counts)),\n",
    "    'entity_types': {k: int(v) for k, v in entity_label_counts.items()},\n",
    "    'most_common_entities': {k: int(v) for k, v in entity_text_counts.most_common(10)},\n",
    "    'processing_errors': int(len(processing_errors))\n",
    "}\n",
    "\n",
    "summary_file = RESULTS_DIR / f'ner_summary_{timestamp}.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Analysis summary saved: {summary_file}\")\n",
    "print(\"\\nNER inference completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae423",
   "metadata": {},
   "source": [
    "## 8. Sample Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc5aa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Results:\n",
      "==================================================\n",
      "\n",
      "Document: doc_193\n",
      "Entities found: 20\n",
      "  - !                    (MISC)\n",
      "  - !                    (LOC)\n",
      "  - /!                   (MISC)\n",
      "  - !                    (LOC)\n",
      "  - /                    (LOC)\n",
      "  ... and 15 more\n",
      "\n",
      "Document: doc_887\n",
      "Entities found: 20\n",
      "  - ---        ---                                                                                                (LOC)\n",
      "  - .                    (LOC)\n",
      "  - s.                   (PER)\n",
      "  - , -                          (LOC)\n",
      "  - -              -            (LOC)\n",
      "  ... and 15 more\n",
      "\n",
      "Document: doc_984\n",
      "Entities found: 19\n",
      "  - !                    (LOC)\n",
      "  - !-                   (LOC)\n",
      "  - !                    (LOC)\n",
      "  - /!-,!-               (PER)\n",
      "  - /!     !             (MISC)\n",
      "  ... and 14 more\n",
      "\n",
      "==================================================\n",
      "Files saved:\n",
      "  - Document results: ner_results_20250831_135758.csv\n",
      "  - Entity details: ner_entities_20250831_135758.csv\n",
      "  - Analysis summary: ner_summary_20250831_135758.json\n",
      "\n",
      "Total processing time for 995 documents completed.\n"
     ]
    }
   ],
   "source": [
    "# Show some sample results\n",
    "print(\"Sample Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find documents with the most entities\n",
    "top_docs = df_with_text.nlargest(3, 'num_entities')\n",
    "\n",
    "for idx, row in top_docs.iterrows():\n",
    "    filename = row.get('filename', f'doc_{idx}')\n",
    "    print(f\"\\nDocument: {filename}\")\n",
    "    print(f\"Entities found: {row['num_entities']}\")\n",
    "    \n",
    "    # Show first few entities\n",
    "    entities = row['entities'][:5]  # First 5 entities\n",
    "    for ent in entities:\n",
    "        print(f\"  - {ent['text']:<20} ({ent['label']})\")\n",
    "    \n",
    "    if len(row['entities']) > 5:\n",
    "        print(f\"  ... and {len(row['entities']) - 5} more\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Files saved:\")\n",
    "print(f\"  - Document results: {results_file.name}\")\n",
    "print(f\"  - Entity details: {entities_file.name}\")\n",
    "print(f\"  - Analysis summary: {summary_file.name}\")\n",
    "\n",
    "print(f\"\\nTotal processing time for {len(df_with_text)} documents completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
