{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF NER Training (French)\n",
    "\n",
    "Train a token classification model (NER) with Hugging Face Transformers on IOB/BIO annotated data.\n",
    "- Supported input: CoNLL (`token\\tPOS?\\tIOB`, blank line between sentences) or CSV with columns: `doc_id, sent_id, token, iob`.\n",
    "- Base model: camembert-base (can swap to xlm-roberta-base).\n",
    "- Saves model + tokenizer to `artifacts/hf_ner_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V\n",
    "import os, json, random, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "ARTIFACTS = Path('artifacts')\n",
    "ARTIFACTS.mkdir(exist_ok=True, parents=True)\n",
    "MODEL_DIR = ARTIFACTS / 'hf_ner_model'\n",
    "DATA_DIR = Path('data')\n",
    "BASE_MODEL = 'camembert-base'  # or 'xlm-roberta-base'\n",
    "print('Artifacts:', MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load IOB data (CoNLL or CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(path: Path):\n",
    "    sents = []\n",
    "    cur = []\n",
    "    for line in path.read_text(encoding='utf-8').splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if cur:\n",
    "                sents.append(cur); cur = []\n",
    "            continue\n",
    "        parts = line.split('\t')\n",
    "        tok = parts[0]\n",
    "        iob = parts[-1] if len(parts) >= 2 else 'O'\n",
    "        cur.append((tok, iob))\n",
    "    if cur:\n",
    "        sents.append(cur)\n",
    "    return sents\n",
    "\n",
    "def read_csv_token_iob(path: Path):\n",
    "    df = pd.read_csv(path, encoding='utf-8')\n",
    "    req = {'doc_id','sent_id','token','iob'}\n",
    "    if not req.issubset(df.columns):\n",
    "        raise ValueError(f'Missing columns in {path}: need {req}')\n",
    "    sents = []\n",
    "    for (doc, sid), grp in df.groupby(['doc_id','sent_id']):\n",
    "        cur = [(str(t), str(i)) for t,i in zip(grp['token'], grp['iob'])]\n",
    "        sents.append(cur)\n",
    "    return sents\n",
    "\n",
    "def load_all(data_dir: Path):\n",
    "    conll_paths = list(data_dir.glob('*.conll')) + list(data_dir.glob('*.txt'))\n",
    "    csv_paths = list(data_dir.glob('*training*.csv')) + list(data_dir.glob('*annotated*.csv'))\n",
    "    sents = []\n",
    "    for p in conll_paths: sents += read_conll(p)\n",
    "    for p in csv_paths: sents += read_csv_token_iob(p)\n",
    "    return sents\n",
    "\n",
    "sents = load_all(DATA_DIR)\n",
    "print('Loaded sentences:', len(sents))\n",
    "# Inspect label distribution\n",
    "cnt = Counter([tag for sent in sents for _,tag in sent if tag!='O'])\n",
    "print('Entity tag counts:', dict(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build HF Datasets (tokens + ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sents:\n",
    "    raise SystemExit('No training data found in data/.')\n",
    "\n",
    "# Label list\n",
    "labels = sorted({tag for sent in sents for _,tag in sent})\n",
    "if 'O' in labels: labels.remove('O')\n",
    "labels = ['O'] + labels\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "print('Labels:', labels)\n",
    "\n",
    "# Split train/val\n",
    "idx = np.arange(len(sents))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.9*len(idx))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "def sents_to_dict(sent_list):\n",
    "    return {\n",
    "        'tokens': [[tok for tok,_ in s] for s in sent_list],\n",
    "        'ner_tags': [[label2id[tag] for _,tag in s] for s in sent_list]\n",
    "    }\n",
    "\n",
    "train_ds = Dataset.from_dict(sents_to_dict([sents[i] for i in train_idx]))\n",
    "val_ds = Dataset.from_dict(sents_to_dict([sents[i] for i in val_idx]))\n",
    "datasets = DatasetDict({'train': train_ds, 'validation': val_ds})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Tokenize and align labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized = tokenizer(examples['tokens'], is_split_into_words=True, truncation=True)\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        prev = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            elif wid != prev:\n",
    "                aligned.append(labels[wid])\n",
    "            else:\n",
    "                # Inside a word -> use I- if B-/I- mapping is needed. Keep same tag here.\n",
    "                aligned.append(labels[wid])\n",
    "            prev = wid\n",
    "        new_labels.append(aligned)\n",
    "    tokenized['labels'] = new_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('seqeval')\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    BASE_MODEL, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
    ")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(MODEL_DIR),\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, lab in zip(preds, labels):\n",
    "        cur_preds, cur_labels = [], []\n",
    "        for p_i, l_i in zip(pred, lab):\n",
    "            if l_i != -100:\n",
    "                cur_preds.append(id2label[int(p_i)])\n",
    "                cur_labels.append(id2label[int(l_i)])\n",
    "        true_preds.append(cur_preds)\n",
    "        true_labels.append(cur_labels)\n",
    "    res = metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        'precision': res.get('overall_precision', 0),\n",
    "        'recall': res.get('overall_recall', 0),\n",
    "        'f1': res.get('overall_f1', 0),\n",
    "        'accuracy': res.get('overall_accuracy', 0),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(str(MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(MODEL_DIR))\n",
    "print('Saved model to', MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
