{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47927df",
   "metadata": {},
   "source": [
    "# Post-Extraction NLP Pipeline\n",
    "\n",
    "This notebook processes extracted PDF text into searchable chunks with embeddings and a simple retriever.\n",
    "\n",
    "Pipeline\n",
    "- Load latest extraction CSV from `output/`\n",
    "- Clean + normalize text\n",
    "- Deduplicate documents\n",
    "- Chunk into overlapping segments\n",
    "- Embed with TF‑IDF (offline) or Sentence-Transformers (optional)\n",
    "- Fit a cosine Nearest Neighbors index\n",
    "- Save artifacts and provide a search helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23cb024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Dev\\ISCOD\\Memoire\\spoti\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy available: True\n",
      "Sentence-Transformers available: True\n"
     ]
    }
   ],
   "source": [
    "# Imports (all are commonly available; Sentence-Transformers is optional)\n",
    "import os, sys, re, json, math, glob, time, hashlib, pathlib, datetime as dt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    \n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SBERT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SBERT_AVAILABLE = False\n",
    "    \n",
    "# For TF-IDF and neighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"SpaCy available: {SPACY_AVAILABLE}\")\n",
    "print(f\"Sentence-Transformers available: {SBERT_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2d81c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use tfidf embeddings\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = Path('output')\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Text processing params\n",
    "MIN_TEXT_LENGTH = 50\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Embedding choice: 'tfidf' or 'sbert'\n",
    "EMBEDDING_METHOD = 'tfidf'\n",
    "\n",
    "# If using SBERT\n",
    "SBERT_MODEL = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "print(f\"Will use {EMBEDDING_METHOD} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620ae84",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dd4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning and normalization.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace non-breaking spaces\n",
    "    text = text.replace('\\\\xa0', ' ')\n",
    "    \n",
    "    # Normalize multiple newlines\n",
    "    text = re.sub(r'\\\\n\\\\s*\\\\n+', '\\\\n\\\\n', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'[ \\\\t]+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"More aggressive normalization for deduplication.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and extra spaces\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', ' ', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def simple_sentence_split(text):\n",
    "    \"\"\"Simple sentence splitting for chunking.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split on sentence boundaries\n",
    "    parts = re.split(r'(?<=[.!?])\\\\s+(?=[A-ZÉÈÀÂÎÔÙ])', text)\n",
    "    return [p.strip() for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e819db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_hash(text):\n",
    "    \"\"\"Generate a hash for text deduplication.\"\"\"\n",
    "    normalized = normalize_text(text)\n",
    "    return hashlib.md5(normalized.encode()).hexdigest()\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    if not text or len(text) < chunk_size:\n",
    "        return [text] if text else []\n",
    "    \n",
    "    # Try sentence-aware chunking first\n",
    "    sentences = simple_sentence_split(text)\n",
    "    \n",
    "    if not sentences:\n",
    "        # Fallback to character-based chunking\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "    \n",
    "    # Sentence-aware chunking\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk.strip():\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581c09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_csv_in_output(pattern='*with_content*.csv'):\n",
    "    \"\"\"Find the most recent CSV file in output directory.\"\"\"\n",
    "    paths = sorted(OUTPUT_DIR.glob(pattern))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f'No CSV found in {OUTPUT_DIR} matching {pattern}')\n",
    "    return paths[-1]  # Most recent\n",
    "\n",
    "def save_artifacts(chunks_df, embeddings, index, metadata):\n",
    "    \"\"\"Save all pipeline artifacts.\"\"\"\n",
    "    timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save chunks DataFrame\n",
    "    chunks_path = RESULTS_DIR / f'chunks_{timestamp}.csv'\n",
    "    chunks_df.to_csv(chunks_path, index=False)\n",
    "    \n",
    "    # Save embeddings\n",
    "    embeddings_path = RESULTS_DIR / f'embeddings_{timestamp}.npy'\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    \n",
    "    # Save index\n",
    "    import pickle\n",
    "    index_path = RESULTS_DIR / f'index_{timestamp}.pkl'\n",
    "    with open(index_path, 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = RESULTS_DIR / f'metadata_{timestamp}.json'\n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Artifacts saved with timestamp {timestamp}\")\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df5a63",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f95289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: output\\pdf_extraction_results_20250830_131547_with_content.csv\n",
      "Loaded 998 documents\n",
      "Columns: ['file_name', 'file_path', 'file_size_kb', 'extraction_timestamp', 'content_type', 'page_count', 'extracted_text', 'has_text', 'has_images', 'image_count', 'word_count', 'char_count', 'paragraph_count', 'line_count', 'error']\n",
      "Using text column: extracted_text\n",
      "Non-empty texts: 997\n"
     ]
    }
   ],
   "source": [
    "# Load the latest extraction CSV\n",
    "csv_path = latest_csv_in_output('*with_content*.csv')\n",
    "print(f\"Loading: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Basic info\n",
    "if 'extracted_text' in df.columns:\n",
    "    text_col = 'extracted_text'\n",
    "elif 'content' in df.columns:\n",
    "    text_col = 'content'\n",
    "else:\n",
    "    text_col = df.columns[-1]  # Assume last column is text\n",
    "\n",
    "print(f\"Using text column: {text_col}\")\n",
    "print(f\"Non-empty texts: {df[text_col].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a931cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "After filtering (min 50 chars): 996 documents\n",
      "Average text length: 3514 chars\n",
      "Median text length: 2427 chars\n",
      "After filtering (min 50 chars): 996 documents\n",
      "Average text length: 3514 chars\n",
      "Median text length: 2427 chars\n"
     ]
    }
   ],
   "source": [
    "# Clean and filter\n",
    "print(\"Cleaning text...\")\n",
    "df['clean_text'] = df[text_col].apply(clean_text)\n",
    "\n",
    "# Filter by minimum length\n",
    "df['text_length'] = df['clean_text'].str.len()\n",
    "df_filtered = df[df['text_length'] >= MIN_TEXT_LENGTH].copy()\n",
    "\n",
    "print(f\"After filtering (min {MIN_TEXT_LENGTH} chars): {len(df_filtered)} documents\")\n",
    "print(f\"Average text length: {df_filtered['text_length'].mean():.0f} chars\")\n",
    "print(f\"Median text length: {df_filtered['text_length'].median():.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69f046",
   "metadata": {},
   "source": [
    "## 2. Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2501e62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating...\n",
      "Removed 54 duplicate documents\n",
      "Remaining: 942 unique documents\n",
      "Removed 54 duplicate documents\n",
      "Remaining: 942 unique documents\n"
     ]
    }
   ],
   "source": [
    "# Add text hashes for deduplication\n",
    "print(\"Deduplicating...\")\n",
    "df_filtered['text_hash'] = df_filtered['clean_text'].apply(text_hash)\n",
    "\n",
    "# Remove duplicates based on text hash\n",
    "before_dedup = len(df_filtered)\n",
    "df_dedup = df_filtered.drop_duplicates(subset=['text_hash']).copy()\n",
    "after_dedup = len(df_dedup)\n",
    "\n",
    "print(f\"Removed {before_dedup - after_dedup} duplicate documents\")\n",
    "print(f\"Remaining: {after_dedup} unique documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5c0fa",
   "metadata": {},
   "source": [
    "## 3. Create Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491f7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking with size=512, overlap=50...\n",
      "Created 942 chunks from 942 documents\n",
      "Average chunk length: 3505 chars\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "print(f\"Chunking with size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}...\")\n",
    "\n",
    "chunks_data = []\n",
    "for idx, row in df_dedup.iterrows():\n",
    "    text = row['clean_text']\n",
    "    chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            'doc_id': row.get('filename', f'doc_{idx}'),\n",
    "            'chunk_id': f\"{row.get('filename', f'doc_{idx}')}_{chunk_idx}\",\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_length': len(chunk),\n",
    "            'chunk_index': chunk_idx\n",
    "        }\n",
    "        \n",
    "        # Copy relevant metadata\n",
    "        for col in ['filename', 'page_count', 'extraction_time']:\n",
    "            if col in row:\n",
    "                chunk_data[col] = row[col]\n",
    "        \n",
    "        chunks_data.append(chunk_data)\n",
    "\n",
    "chunks_df = pd.DataFrame(chunks_data)\n",
    "print(f\"Created {len(chunks_df)} chunks from {len(df_dedup)} documents\")\n",
    "print(f\"Average chunk length: {chunks_df['chunk_length'].mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87ab4f",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380c8d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating tfidf embeddings...\n",
      "Embeddings shape: (942, 5000)\n",
      "Embeddings shape: (942, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings based on chosen method\n",
    "print(f\"Generating {EMBEDDING_METHOD} embeddings...\")\n",
    "\n",
    "if EMBEDDING_METHOD == 'tfidf':\n",
    "    # TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words=None,  # Keep French stopwords if needed\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    \n",
    "    embeddings = vectorizer.fit_transform(chunks_df['chunk_text']).toarray()\n",
    "    embedding_model = vectorizer\n",
    "    \n",
    "elif EMBEDDING_METHOD == 'sbert' and SBERT_AVAILABLE:\n",
    "    # Sentence-BERT embeddings\n",
    "    model = SentenceTransformer(SBERT_MODEL)\n",
    "    embeddings = model.encode(chunks_df['chunk_text'].tolist(), show_progress_bar=True)\n",
    "    embedding_model = model\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Embedding method '{EMBEDDING_METHOD}' not available or supported\")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a11ff",
   "metadata": {},
   "source": [
    "## 5. Build Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e139c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building search index...\n",
      "Search index built successfully\n"
     ]
    }
   ],
   "source": [
    "# Build nearest neighbors index\n",
    "print(\"Building search index...\")\n",
    "\n",
    "# Use cosine similarity\n",
    "index = NearestNeighbors(\n",
    "    n_neighbors=min(10, len(chunks_df)),\n",
    "    metric='cosine',\n",
    "    algorithm='brute'  # Better for small datasets\n",
    ")\n",
    "\n",
    "index.fit(embeddings)\n",
    "print(\"Search index built successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f7844",
   "metadata": {},
   "source": [
    "## 6. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf29030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved with timestamp 20250831_154154\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'timestamp': dt.datetime.now().isoformat(),\n",
    "    'source_csv': str(csv_path),\n",
    "    'num_documents': len(df_dedup),\n",
    "    'num_chunks': len(chunks_df),\n",
    "    'embedding_method': EMBEDDING_METHOD,\n",
    "    'embedding_dim': embeddings.shape[1],\n",
    "    'chunk_size': CHUNK_SIZE,\n",
    "    'chunk_overlap': CHUNK_OVERLAP,\n",
    "    'min_text_length': MIN_TEXT_LENGTH\n",
    "}\n",
    "\n",
    "if EMBEDDING_METHOD == 'sbert':\n",
    "    metadata['sbert_model'] = SBERT_MODEL\n",
    "\n",
    "# Save everything\n",
    "timestamp = save_artifacts(chunks_df, embeddings, index, metadata)\n",
    "print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e706045",
   "metadata": {},
   "source": [
    "## 7. Search Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ce6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing search functionality...\n",
      "\\nSearch results for 'emploi formation':\n",
      "\\n1. Similarity: 0.137\n",
      "Document: doc_133\n",
      "Text: DIRECTION DES RESSOURCES HUMAINES \n",
      "\n",
      "SERVICE EMPLOI, FORMATION ET PARCOURS PROFESSIONNELS \n",
      "\n",
      "SECTEUR RECRUTEMENT – MOBILITE – EVOLUTION PROFESSIONNELLE \n",
      "\n",
      "UN.E AGENT.E ADMINISTRATIF.VE ET D'ACCUEIL \n",
      "\n",
      "Cab...\n",
      "\\n2. Similarity: 0.121\n",
      "Document: doc_882\n",
      "Text: DIRECTION DES RESSOURCES HUMAINES \n",
      "\n",
      "SERVICE EMPLOI, FORMATION ET PARCOURS PROFESSIONNELS \n",
      "\n",
      "SECTEUR RECRUTEMENT – MOBILITE – EVOLUTION PROFESSIONNELLE \n",
      "\n",
      "LA VILLE DE METZ, \n",
      "118 634 habi an s, ville-cen ...\n",
      "\\n3. Similarity: 0.116\n",
      "Document: doc_486\n",
      "Text: DIRECTION DES RESSOURCES HUMAINES \n",
      "\n",
      "SERVICE EMPLOI, FORMATION ET PARCOURS PROFESSIONNELS \n",
      "\n",
      "SECTEUR RECRUTEMENT – MOBILITE – EVOLUTION PROFESSIONNELLE \n",
      "\n",
      "LA VILLE DE METZ, \n",
      "118 634 habi an s, ville-cen ...\n"
     ]
    }
   ],
   "source": [
    "def search_documents(query, k=5):\n",
    "    \"\"\"Search for relevant chunks using the trained index.\"\"\"\n",
    "    if EMBEDDING_METHOD == 'tfidf':\n",
    "        query_embedding = embedding_model.transform([query]).toarray()\n",
    "    elif EMBEDDING_METHOD == 'sbert':\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # Find nearest neighbors\n",
    "    distances, indices = index.kneighbors(query_embedding, n_neighbors=k)\n",
    "    \n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        chunk_info = chunks_df.iloc[idx]\n",
    "        similarity = 1 - dist  # Convert distance to similarity\n",
    "        \n",
    "        results.append({\n",
    "            'rank': i + 1,\n",
    "            'similarity': similarity,\n",
    "            'doc_id': chunk_info['doc_id'],\n",
    "            'chunk_id': chunk_info['chunk_id'],\n",
    "            'text': chunk_info['chunk_text'][:200] + '...' if len(chunk_info['chunk_text']) > 200 else chunk_info['chunk_text']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "print(\"Testing search functionality...\")\n",
    "test_query = \"emploi formation\"\n",
    "test_results = search_documents(test_query, k=3)\n",
    "\n",
    "print(f\"\\\\nSearch results for '{test_query}':\")\n",
    "for result in test_results:\n",
    "    print(f\"\\\\n{result['rank']}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"Document: {result['doc_id']}\")\n",
    "    print(f\"Text: {result['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
