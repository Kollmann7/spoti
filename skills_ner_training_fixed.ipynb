{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6dad9b",
   "metadata": {},
   "source": [
    "# NER Training from Manually Annotated Job Ads\n",
    "\n",
    "Train a French NER model from a corpus annotated with IOB/BIO tags (general job ads, not only tech).\n",
    "- Accepts either CoNLL-like files (`token\\tPOS?\\tIOB`, sentence-separated by blank lines) or CSV with columns: `doc_id, sent_id, token, iob` (optional `pos`).\n",
    "- Builds spaCy docs and trains a small NER head.\n",
    "- Saves model to `artifacts/ner_custom`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd773fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed successfully\n",
      "SpaCy version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import datetime as dt\n",
    "\n",
    "# SpaCy imports\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "\n",
    "print(\"Imports completed successfully\")\n",
    "print(f\"SpaCy version: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to: artifacts\\ner_custom\n",
      "Training data directory: data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_OUTPUT_DIR = ARTIFACTS_DIR / 'ner_custom'\n",
    "DATA_DIR = Path('data')  # Assume training data is here\n",
    "\n",
    "# Training parameters\n",
    "TRAINING_ITERATIONS = 30\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Model will be saved to: {MODEL_OUTPUT_DIR}\")\n",
    "print(f\"Training data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed43ce",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a1665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_file(file_path):\n",
    "    \"\"\"Load data from CoNLL format file (token\\tPOS\\tIOB).\"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:  # Empty line = sentence boundary\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    token = parts[0]\n",
    "                    iob = parts[-1]  # Last column is IOB tag\n",
    "                    pos = parts[1] if len(parts) >= 3 else None\n",
    "                    current_sentence.append((token, pos, iob))\n",
    "    \n",
    "    # Add last sentence if file doesn't end with empty line\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def load_csv_file(file_path):\n",
    "    \"\"\"Load data from CSV format (doc_id, sent_id, token, iob, pos?).\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    required_cols = ['doc_id', 'sent_id', 'token', 'iob']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"CSV must contain columns: {required_cols}\")\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # Group by document and sentence\n",
    "    for (doc_id, sent_id), group in df.groupby(['doc_id', 'sent_id']):\n",
    "        sentence = []\n",
    "        for _, row in group.iterrows():\n",
    "            token = row['token']\n",
    "            iob = row['iob']\n",
    "            pos = row.get('pos', None)\n",
    "            sentence.append((token, pos, iob))\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def convert_iob_to_spacy(sentences):\n",
    "    \"\"\"Convert IOB format to spaCy training format.\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = [token for token, pos, iob in sentence]\n",
    "        iob_tags = [iob for token, pos, iob in sentence]\n",
    "        \n",
    "        # Reconstruct text\n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        # Convert IOB to character-based entities\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        char_offset = 0\n",
    "        \n",
    "        for i, (token, tag) in enumerate(zip(tokens, iob_tags)):\n",
    "            if tag.startswith('B-'):\n",
    "                # Begin new entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = tag[2:]\n",
    "                current_entity = {\n",
    "                    'start': char_offset,\n",
    "                    'end': char_offset + len(token),\n",
    "                    'label': entity_type\n",
    "                }\n",
    "            \n",
    "            elif tag.startswith('I-') and current_entity:\n",
    "                # Continue current entity\n",
    "                current_entity['end'] = char_offset + len(token)\n",
    "            \n",
    "            else:  # O tag or end of entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "            \n",
    "            char_offset += len(token) + 1  # +1 for space\n",
    "        \n",
    "        # Add final entity if exists\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        # Convert to spaCy format\n",
    "        spacy_entities = [(ent['start'], ent['end'], ent['label']) for ent in entities]\n",
    "        training_data.append((text, {'entities': spacy_entities}))\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ea483",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd367e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CoNLL files: []\n",
      "Found CSV files: []\n",
      "WARNING: No training data found!\n",
      "Please add annotated data files to the data/ directory\n",
      "Supported formats:\n",
      "  - CoNLL: token\tpos\tiob (one token per line, empty lines separate sentences)\n",
      "  - CSV: columns doc_id, sent_id, token, iob (optional pos column)\n"
     ]
    }
   ],
   "source": [
    "# Look for training data files\n",
    "conll_files = list(DATA_DIR.glob('*.conll')) + list(DATA_DIR.glob('*.txt'))\n",
    "csv_files = list(DATA_DIR.glob('*training*.csv')) + list(DATA_DIR.glob('*annotated*.csv'))\n",
    "\n",
    "print(f\"Found CoNLL files: {[f.name for f in conll_files]}\")\n",
    "print(f\"Found CSV files: {[f.name for f in csv_files]}\")\n",
    "\n",
    "# Load data from available files\n",
    "all_sentences = []\n",
    "\n",
    "for file_path in conll_files:\n",
    "    print(f\"Loading CoNLL file: {file_path}\")\n",
    "    sentences = load_conll_file(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    print(f\"  Loaded {len(sentences)} sentences\")\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(f\"Loading CSV file: {file_path}\")\n",
    "    sentences = load_csv_file(file_path)\n",
    "    all_sentences.extend(sentences)\n",
    "    print(f\"  Loaded {len(sentences)} sentences\")\n",
    "\n",
    "if not all_sentences:\n",
    "    print(\"WARNING: No training data found!\")\n",
    "    print(\"Please add annotated data files to the data/ directory\")\n",
    "    print(\"Supported formats:\")\n",
    "    print(\"  - CoNLL: token\\tpos\\tiob (one token per line, empty lines separate sentences)\")\n",
    "    print(\"  - CSV: columns doc_id, sent_id, token, iob (optional pos column)\")\n",
    "else:\n",
    "    print(f\"\\nTotal sentences loaded: {len(all_sentences)}\")\n",
    "    \n",
    "    # Show some statistics\n",
    "    total_tokens = sum(len(sent) for sent in all_sentences)\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    \n",
    "    # Count entity types\n",
    "    entity_counts = Counter()\n",
    "    for sentence in all_sentences:\n",
    "        for token, pos, iob in sentence:\n",
    "            if iob != 'O':\n",
    "                entity_type = iob.split('-')[-1]\n",
    "                entity_counts[entity_type] += 1\n",
    "    \n",
    "    print(f\"\\nEntity types found: {dict(entity_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c23c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training data available - skipping training\n"
     ]
    }
   ],
   "source": [
    "# Convert to spaCy format\n",
    "if all_sentences:\n",
    "    print(\"Converting to spaCy format...\")\n",
    "    training_data = convert_iob_to_spacy(all_sentences)\n",
    "    \n",
    "    print(f\"Converted {len(training_data)} examples\")\n",
    "    \n",
    "    # Show first example\n",
    "    if training_data:\n",
    "        text, annotations = training_data[0]\n",
    "        print(f\"\\nFirst example:\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        print(f\"Entities: {annotations['entities'][:5]}...\")\n",
    "    \n",
    "    # Split train/validation\n",
    "    random.seed(42)\n",
    "    random.shuffle(training_data)\n",
    "    \n",
    "    split_point = int(len(training_data) * 0.8)\n",
    "    train_data = training_data[:split_point]\n",
    "    val_data = training_data[split_point:]\n",
    "    \n",
    "    print(f\"\\nTraining examples: {len(train_data)}\")\n",
    "    print(f\"Validation examples: {len(val_data)}\")\n",
    "else:\n",
    "    print(\"No training data available - skipping training\")\n",
    "    train_data = []\n",
    "    val_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c21a1",
   "metadata": {},
   "source": [
    "## 3. Create and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7da3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_data:\n",
    "    # Create a blank French model or load existing one\n",
    "    try:\n",
    "        nlp = spacy.load('fr_core_news_sm')\n",
    "        print(\"Loaded existing French model\")\n",
    "    except OSError:\n",
    "        print(\"French model not found, creating blank model\")\n",
    "        nlp = spacy.blank('fr')\n",
    "    \n",
    "    # Add NER component if not present\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner')\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    \n",
    "    # Add entity labels\n",
    "    entity_labels = set()\n",
    "    for text, annotations in train_data:\n",
    "        for start, end, label in annotations['entities']:\n",
    "            entity_labels.add(label)\n",
    "    \n",
    "    for label in entity_labels:\n",
    "        ner.add_label(label)\n",
    "    \n",
    "    print(f\"Entity labels to train: {sorted(entity_labels)}\")\n",
    "    \n",
    "    # Disable other pipes during training\n",
    "    disabled_pipes = []\n",
    "    for pipe_name in nlp.pipe_names:\n",
    "        if pipe_name != 'ner':\n",
    "            disabled_pipes.append(pipe_name)\n",
    "    \n",
    "    print(f\"Disabled pipes during training: {disabled_pipes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c6f87",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89c4699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping training - no data available\n"
     ]
    }
   ],
   "source": [
    "if train_data:\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Prepare training examples\n",
    "    def create_examples(data):\n",
    "        examples = []\n",
    "        for text, annotations in data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "    \n",
    "    train_examples = create_examples(train_data)\n",
    "    val_examples = create_examples(val_data) if val_data else []\n",
    "    \n",
    "    # Initialize the model\n",
    "    nlp.initialize(lambda: train_examples)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    \n",
    "    with nlp.disable_pipes(*disabled_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        \n",
    "        for iteration in range(TRAINING_ITERATIONS):\n",
    "            print(f\"\\nIteration {iteration + 1}/{TRAINING_ITERATIONS}\")\n",
    "            \n",
    "            # Shuffle training data\n",
    "            random.shuffle(train_examples)\n",
    "            \n",
    "            batch_losses = []\n",
    "            batches = minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))\n",
    "            \n",
    "            for batch in batches:\n",
    "                nlp.update(batch, drop=DROPOUT_RATE, losses={})\n",
    "                batch_losses.append(losses.get('ner', 0.0))\n",
    "            \n",
    "            avg_loss = np.mean(batch_losses) if batch_losses else 0.0\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            print(f\"  Average loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Evaluate on validation set every 5 iterations\n",
    "            if val_examples and (iteration + 1) % 5 == 0:\n",
    "                print(\"  Evaluating on validation set...\")\n",
    "                scores = nlp.evaluate(val_examples)\n",
    "                print(f\"  NER Precision: {scores['ents_p']:.3f}\")\n",
    "                print(f\"  NER Recall: {scores['ents_r']:.3f}\")\n",
    "                print(f\"  NER F1: {scores['ents_f']:.3f}\")\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "else:\n",
    "    print(\"Skipping training - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735a585",
   "metadata": {},
   "source": [
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c548840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model to save - no training data was available\n"
     ]
    }
   ],
   "source": [
    "if train_data:\n",
    "    # Save the trained model\n",
    "    MODEL_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    nlp.to_disk(MODEL_OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"Model saved to: {MODEL_OUTPUT_DIR}\")\n",
    "    \n",
    "    # Save training metadata\n",
    "    metadata = {\n",
    "        'timestamp': dt.datetime.now().isoformat(),\n",
    "        'training_examples': len(train_data),\n",
    "        'validation_examples': len(val_data),\n",
    "        'iterations': TRAINING_ITERATIONS,\n",
    "        'entity_labels': sorted(entity_labels),\n",
    "        'final_loss': losses[-1] if losses else None,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'batch_size': BATCH_SIZE\n",
    "    }\n",
    "    \n",
    "    metadata_file = MODEL_OUTPUT_DIR / 'training_metadata.json'\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Training metadata saved to: {metadata_file}\")\n",
    "else:\n",
    "    print(\"No model to save - no training data was available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fea9e",
   "metadata": {},
   "source": [
    "## 6. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5e76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model to test - training was skipped\n"
     ]
    }
   ],
   "source": [
    "if train_data:\n",
    "    # Test the model with some sample text\n",
    "    test_texts = [\n",
    "        \"Nous recherchons un développeur Python avec une expérience en Django et React.\",\n",
    "        \"Poste d'ingénieur logiciel spécialisé en Java et Spring Boot.\",\n",
    "        \"Analyste de données maîtrisant SQL, R et Tableau.\",\n",
    "        \"Chef de projet agile avec certification Scrum Master.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing the trained model:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        print(f\"\\nTest {i}: {text}\")\n",
    "        print(\"Entities found:\")\n",
    "        \n",
    "        if doc.ents:\n",
    "            for ent in doc.ents:\n",
    "                print(f\"  - {ent.text:<15} ({ent.label_})\")\n",
    "        else:\n",
    "            print(\"  No entities found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Model testing completed!\")\n",
    "    print(f\"\\nTo use this model in other scripts:\")\n",
    "    print(f\"  nlp = spacy.load('{MODEL_OUTPUT_DIR}')\")\n",
    "else:\n",
    "    print(\"No model to test - training was skipped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
